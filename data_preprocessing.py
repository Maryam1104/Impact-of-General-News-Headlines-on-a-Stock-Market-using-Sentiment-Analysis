# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14-LUZdbrW_GgOwo6pe55nPPUKao5qtCP
"""

import pandas as pd

data=pd.read_csv("C:/rmproject/News_Headlines.csv")

data.head(10)

"""# Remove Punctuation"""

import re
s = "string. with. Punctuation?"
s = re.sub(r'[^\w\s]','',s)

s

"""# Tokenization"""

import nltk
nltk.download('punkt')
nltk.download('wordnet')

nltk.word_tokenize("hi how are you")

"""# Remove Stop words"""

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
print(stop_words)

"""# Stemmer"""

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
input_str= "There are several types of stemming algorithms."
input_str=nltk.word_tokenize(input_str)
for word in input_str:
    print(stemmer.stem(word))

"""# Lemmatization"""

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
input_str= "been had done languages cities mice"
input_str=nltk.word_tokenize(input_str)
for word in input_str:
    print(lemmatizer.lemmatize(word))

"""# Applying preprocessing"""

data['News'][0:10]

lemmatizer=WordNetLemmatizer()
for index,row in data.iterrows():
    filter_sentence = []
    sentence = row['News']
    sentence = re.sub(r'[^\w\s]','',str(sentence)) #cleaning
    words = nltk.word_tokenize(sentence) #tokenization
    words = [w for w in words if not w in stop_words] #stopwords removal
    for word in words:
        filter_sentence.append(lemmatizer.lemmatize(word))
    print(filter_sentence)
    data.loc[index, "News"] = filter_sentence

#df = pd.DataFrame(filter_sentence)
#df
#df.to_csv('myfile.csv')

